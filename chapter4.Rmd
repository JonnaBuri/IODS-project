---
title: "chapter4"
author: "Ioanna Bouri"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Week 4

## Task 2: Reading the data

```{r}
library(MASS)
data("Boston")

# explore the dataset
dim(Boston)
```

Boston data is a collection of 506 records including information about criminal, educational and demographical factors in Boston:

- **crim: ** per capita crime rate by town.

- **zn:** proportion of residential land zoned for lots over 25,000 sq.ft.

- **indus:** proportion of non-retail business acres per town.

- **chas:** Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

- **nox:** nitrogen oxides concentration (parts per 10 million).

- **rm:** average number of rooms per dwelling.

- **age:** proportion of owner-occupied units built prior to 1940.

- **dis:** weighted mean of distances to five Boston employment centres.

- **rad:** index of accessibility to radial highways.

- **tax:** full-value property-tax rate per \$10,000.

- **ptratio:** pupil-teacher ratio by town.

- **black:** 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

- **lstat:** lower status of the population (percent).

- **medv:** median value of owner-occupied homes in \$1000s.

## Task 3: Exploratory data analysis

```{r}
str(Boston)
```

```{r}
summary(Boston)
```

```{r}
pairs(Boston)
```

Let's explore the correlation of criminality rate per capita, with the variables available. (two plots so that they are more readable)

```{r}
library(GGally)
library(ggplot2)

mycols <- c("crim", "zn", "indus", "nox", "rm", "age", "dis")
ggpairs(Boston[mycols], mapping = aes(), lower = list(combo = wrap("facethist", bins = 20)))
```


```{r}
mycols <- c("crim", "rad", "tax", "ptratio", "black", "lstat", "medv")
ggpairs(Boston[mycols], mapping = aes(), lower = list(combo = wrap("facethist", bins = 20)))
```

Interestingly, we can point out many strong correlations from the data. For instance, some observations that seem interesting to me, is that the more industrial an area is, the less residential plots there are, and the higher the nitrogen oxides concentration. Another interesting correlation to investigate is, that higher tax rates seem to be positively correlated with a higher pupil to teacher ratio.

We can see that we have different kinds of distributions for variables, some being normally distributed (i.e. rm - meant to represent average values of rooms per dwelling), we have binary variables (i.e. chas), and many that stand for percentages or ratios.

We can plot the correlations more clearly:

```{r}
library(tidyr)
library(corrplot)

cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)
```


## Task 4: Scaling the data

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# summaries of the scaled variables
summary(boston_scaled)
```

After standardizing the data, we can observe that all the variable distributions are zero-centered.

### Creating a crime rate categorical variable

```{r}
# summary of the scaled crime rate
summary(boston_scaled$crim)
```

```{r}
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)
```

We separated the data in low - med_low - med_high - high categories.

### Replace original crime rate variable from dataframe with the categorical

```{r}
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
summary(boston_scaled)
```

### Divide the dataset to train and test sets

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]
```

## Task 5: Fitting the LDA

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit
```


```{r}
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)

```

## Task 6: Predictions

```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

From the table we can see that mostly the classifier is doing a good job. We can observe only some difficulty in classifying the data from the class med_low, sometimes they appear to be misclassified as low, and less often as med_high too.

## Task 7: K-means clustering

### Reading and scaling the data

```{r , echo=FALSE}
data('Boston')
data_scaled <- scale(Boston)
summary(data_scaled)
```

### Distance matrix (Euclidean)

```{r , echo=FALSE}
# euclidean distance matrix
dist_eu <- dist(data_scaled)

# look at the summary of the distances
summary(dist_eu)
```

### K-means

```{r , echo=FALSE}
# k-means clustering
km <-kmeans(data_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(data_scaled, col = km$cluster)
```

### Searching for a good k

```{r , echo=FALSE}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(data_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')


```

We can determine the k by the number of WCSS dropping radically, here this happens when k = 2.

```{r , echo=FALSE}
# k-means clustering
km <-kmeans(data_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(data_scaled, col = km$cluster)
```

In most pairs we can see that the cluster separation works with classifying the data with k=2.